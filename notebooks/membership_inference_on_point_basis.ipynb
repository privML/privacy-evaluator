{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Membership Inference Attack On Point Basis (Privacy Risk Score)"
   ],
   "metadata": {
    "id": "Rf_QO8flWx6_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/privML/privacy-evaluator/blob/main/notebooks/membership_inference_on_point_basis.ipynb\"><img src=\"https://raw.githubusercontent.com/privML/privacy-evaluator/team1sprint5/notebooks/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/privML/privacy-evaluator/blob/main/notebooks/membership_inference_on_point_basis.ipynb\"><img src=\"https://raw.githubusercontent.com/privML/privacy-evaluator/main/notebooks/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ],
   "metadata": {
    "id": "WKAYn2M4Wx7F"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we want to show you how to use the `privacy-evaluator` tool to perform the Membership Inference Attacks On Point Basis also known as calculating the privacy risk score [1] on both, a provided PyTorch and a provided Tensorflow model.\n"
   ],
   "metadata": {
    "id": "Ia0VmlzeWx7H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "First, you should set the notebook's runtime to use a GPU (e.g. if Colab is used go to ***Runtime > Change runtime type > Hardware accelerator***). Now we can install the `privacy-evaluator` package and import all needed modules."
   ],
   "metadata": {
    "id": "DH6rB9KSWx7J"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip3 install git+https://github.com/privML/privacy-evaluator"
   ],
   "outputs": [],
   "metadata": {
    "id": "lNU7L3GCWx7M"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "import torch\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "import tensorflow.python.ops.numpy_ops.np_config as np_config\r\n",
    "np_config.enable_numpy_behavior()\r\n",
    "\r\n",
    "import privacy_evaluator.models.torch.dcti.dcti as torch_dcti\r\n",
    "import privacy_evaluator.models.tf.dcti.dcti as tf_dcti \r\n",
    "\r\n",
    "from privacy_evaluator.datasets.tf.cifar10 import TFCIFAR10\r\n",
    "from privacy_evaluator.datasets.torch.cifar10 import TorchCIFAR10\r\n",
    "\r\n",
    "from privacy_evaluator.classifiers.classifier import Classifier\r\n",
    "\r\n",
    "from privacy_evaluator.attacks.membership_inference.on_point_basis import MembershipInferenceAttackOnPointBasis\r\n",
    "from privacy_evaluator.attacks.membership_inference import MembershipInferenceAttackAnalysis\r\n",
    "\r\n",
    "from privacy_evaluator.output.user_output_privacy_score import UserOutputPrivacyScore\r\n",
    "\r\n",
    "from privacy_evaluator.attacks.membership_inference.membership_inference_point_analysis import MembershipInferencePointAnalysis\r\n",
    "\r\n",
    "from privacy_evaluator.attacks.membership_inference.data_structures.attack_input_data import AttackInputData\r\n",
    "from privacy_evaluator.attacks.membership_inference.data_structures.slicing import Slicing"
   ],
   "outputs": [],
   "metadata": {
    "id": "9d7ab468",
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conduct Membership Inference Attack On Point Basis\n",
    "\n",
    "Now we can start with conducting the Membership Inference Attacks On Point Basis. Therefore, we prepared two instances of the attack: one attacking a PyTorch model and attacking a TensorFlow model. For both attacks, we implemented a simple neural network trained on the CIFAR-10 dataset. For details about the provided network have a look at the following paper: https://www.scitepress.org/Papers/2018/67520/67520.pdf)."
   ],
   "metadata": {
    "id": "mUH4YQ4tWx7Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### PyTorch\n",
    "\n",
    "We start the evaluation of the PyTorch version of the model."
   ],
   "metadata": {
    "id": "b2da0876"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare target model\n",
    "\n",
    "Now, we need to initialize our pre-trained Lightweight Deep Convolutional Neural Network (short DCTI) as a generic `Classifier`. Therefore we need to specify the loss function used to train the model (in our case the `torch.nn.CrossEntropyLoss`), the number of classes and the input shape of our CIFAR-10 dataset."
   ],
   "metadata": {
    "id": "7aQIN6UMWx7W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initalize PyTorch model as a Classifier\r\n",
    "target_model = Classifier(\r\n",
    "    torch_dcti.load_dcti(), # PyTorch DCTI \r\n",
    "    loss=torch.nn.CrossEntropyLoss(reduction=\"none\"), # Loss function of the PyTorch model\r\n",
    "    nb_classes=TorchCIFAR10.N_CLASSES, # Number of classes of the CIFAR10 dataset\r\n",
    "    input_shape=TorchCIFAR10.INPUT_SHAPE # Input shape of the CIFAR10 dataset\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "fz4UuSxMWx7Y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load CIFAR10 Dataset\n",
    "\n",
    "Before we can start to conduct the membership inference attacks, we need to load the dataset. The CIFAR10 dataset needs to be preprocessed in a specific manner to work for the PyTorch model."
   ],
   "metadata": {
    "id": "sv_fGg8jWx7S"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load CIFAR10 dataset as numpy array\r\n",
    "x_train, y_train, x_test, y_test = TorchCIFAR10.numpy()"
   ],
   "outputs": [],
   "metadata": {
    "id": "dcae11d8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Perform Membership Attack on Point Basis\n",
    "\n",
    "Next, we want to perform a Membership Inference Attack on Point Basis its output is also known as Privacy Risk Score [1]. In this case, we do not need to fit the attack because this approach depends only on the attacked the loss calculation of the data points and the target model.\n",
    "\n",
    "To do so, we input the target model and the data points which should be evaluated to the respective function. The given data points are separated into a train and tests set. The train set contains of the data (`x_train`) and its corresponding labels (`y_train`) which were used to train the target model. The test set contains the data (`x_test`) and its corresponding labels (`y_test`) which were not part of the training process of the target model. As a result, we get privacy risk scores for each data point, separated into train and test scores. The resulting values indicate the probability of a data point being a member or not."
   ],
   "metadata": {
    "id": "76a432f6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "attack = MembershipInferenceAttackOnPointBasis(target_model)\r\n",
    "# Compute membership privacy risk score for the PyTorch model\r\n",
    "(train_privacy_risk_score, test_privacy_risk_score) = attack.attack(x_train[:400], y_train[:400], x_test[:400], y_test[:400])"
   ],
   "outputs": [],
   "metadata": {
    "id": "GzT8PRx3EIVc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Get human-readable privacy risk score statistics\n",
    "\n",
    "We can create more human-readable privacy risk score statistics. Therefore we generate an output by providing the privacy risk scores and the true labels of the data points for which we computed the privacy risk scores. This output can then be visualized in two separate ways."
   ],
   "metadata": {
    "id": "CQdBbr5CEKo6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create user output and plot histogram for train dataset\r\n",
    "output = UserOutputPrivacyScore(\r\n",
    "    np.argmax(y_train[:400], axis=1),\r\n",
    "    train_privacy_risk_score, \r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "EEjwlA03ELD6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first way to visualise the privacy risk scores is as a histogram. The histogram shows the distribution of the k-top data points with the highest privacy risk scores per class."
   ],
   "metadata": {
    "id": "fgz5fsp0EOTO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot absolut values \r\n",
    "labels, count = output.histogram_top_k(range(10), 50)"
   ],
   "outputs": [],
   "metadata": {
    "id": "tvSypW8HEOzv"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a second option, you can visualise the privacy risk scores again as a histogram of the distribution of the k-top data points with the highest privacy risk scores per class, but this time the values are relative to the size of respective classe"
   ],
   "metadata": {
    "id": "0-zzVnlbERPK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot relative values \r\n",
    "labels, count = output.histogram_top_k_relative(range(10), 50)"
   ],
   "outputs": [],
   "metadata": {
    "id": "78e80HEmERr7"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "analysis = MembershipInferencePointAnalysis(\r\n",
    "    AttackInputData(x_train[:100], y_train[:100], x_test[:100], y_test[:100])\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "slicing = Slicing(True, False, True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "analysis.analyse(target_model, slicing)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation of the outcome:\n",
    "\n",
    "##### Vulnerability of individual data points:\n",
    "The privacy risk score is an individual sample’s likelihood of being a training member, which allows an adversary to identify samples with high privacy risks and perform membership inference attacks with high confidence [1].\n",
    "\n",
    "The training data points that exhibit an increased membership privacy risk might differ from their classes mean samples (outliers) [2]. You could check them again, see if they have the correct label, or if they exhibit any non-standard properties for the class. If so, correct them. It was also shown that points with an high influence on the decision boundary are more vulnerable to membership inference attacks [3]. Therefore, these points might be important. If you want to protect them, you might add several similar training samples as they are to the class. \n",
    "\n",
    "\n",
    "(For References, please see last box)"
   ],
   "metadata": {
    "id": "aPdWNxlgFOyU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TensorFlow\n",
    "\n",
    "Now we do the same with the TensorFlow model."
   ],
   "metadata": {
    "id": "xG4nYKGzWx7i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prepare target model\n",
    "\n",
    "Now, we need to initialize our pre-trained Lightweight Deep Convolutional Neural Network (short DCTI) as a generic `Classifier`. Therefore we need to specify the loss function used to train the model (in our case the `tf.keras.losses.CategoricalCrossentropy`), the number of classes and the input shape of our CIFAR-10 dataset."
   ],
   "metadata": {
    "id": "_7ty54AKWx7k"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Initalize TensorFlow target model\r\n",
    "target_model = Classifier(\r\n",
    "    tf_dcti.load_dcti(), # TensorFlow DCTI\r\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(), # Loss function of the TensorFlow target model\r\n",
    "    nb_classes=TFCIFAR10.N_CLASSES, # Number of classes of the CIFAR10 dataset\r\n",
    "    input_shape=TFCIFAR10.INPUT_SHAPE # Input shape of the CIFAR10 dataset\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "Uhrn5znOWx7k"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load CIFAR10 Dataset\n",
    "\n",
    "Again, before we can start to conduct the membership inference attacks, we need to load the dataset. The CIFAR10 dataset needs to be preprocessed in a specific manner to work for the TensorFlow model."
   ],
   "metadata": {
    "id": "UBfeudg_Wx7i"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Load CIFAR10 dataset as numpy array\r\n",
    "x_train, y_train, x_test, y_test = TFCIFAR10.numpy()"
   ],
   "outputs": [],
   "metadata": {
    "id": "kAJgfSgoWx7j"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Perform Membership Attack on Point Basis\n",
    "\n",
    "Next, we want to perform a Membership Inference Attack on Point Basis its output also known as Privacy Risk Score [1]. In this case, we do not need to fit the attack because this approach depends only on the attacked the loss calculation of the data points and the target model.\n",
    "\n",
    "To do so, we input the target model and the data points which should be evaluated to the respective function. The given data points are separated into a train and tests set. The train set contains of the data (`x_train`) and its corresponding labels (`y_train`) which were used to train the target model. The test set contains the data (`x_test`) and its corresponding labels (`y_test`) which were not part of the training process of the target model. As a result, we get privacy risk scores for each data point, separated into train and test scores. The resulting values indicate the probability of a data point being a member or not."
   ],
   "metadata": {
    "id": "cmEM23LTEpmo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "attack = MembershipInferenceAttackOnPointBasis(target_model)\r\n",
    "# Compute membership privacy risk score for the PyTorch model\r\n",
    "(train_privacy_risk_score, test_privacy_risk_score) = attack.attack(x_train[:100], y_train[:100], x_test[:100], y_test[:100])"
   ],
   "outputs": [],
   "metadata": {
    "id": "tD9B3Es2EsEc"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Get human-readable privacy risk score statistics\n",
    "\n",
    "We can create more human-readable privacy risk score statistics. Therefore we generate an output by providing the privacy risk scores and the true labels of the data points for which we computed the privacy risk scores. This output can then be visualized in two separate ways."
   ],
   "metadata": {
    "id": "4LzAETqNEtmf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create user output and plot histogram for train dataset\r\n",
    "output = UserOutputPrivacyScore(\r\n",
    "    np.argmax(y_train[:100], axis=1), \r\n",
    "    train_privacy_risk_score, \r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "f6Se9E15Evx2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first way to visualise the privacy risk scores is as a histogram. The histogram shows the distribution of the k-top data points with the highest privacy risk scores per class."
   ],
   "metadata": {
    "id": "6IdryTLMExbv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot absolut values \r\n",
    "labels, count = output.histogram_top_k(range(10), 50)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Y_OzKCPhEx6V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a second option, you can visualise the privacy risk scores again as a histogram of the distribution of the k-top data points with the highest privacy risk scores per class, but this time the values are relative to the size of respective classe"
   ],
   "metadata": {
    "id": "AAtwXb71Ey0_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Plot relative values \r\n",
    "labels, count = output.histogram_top_k_relative(range(10), 50)"
   ],
   "outputs": [],
   "metadata": {
    "id": "Z-ImMLkyEzG0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explanation of the outcome:\n",
    "\n",
    "##### Vulnerability of individual data points:\n",
    "The privacy risk score is an individual sample’s likelihood of being a training member, which allows an adversary to identify samples with high privacy risks and perform membership inference attacks with high confidence [1].\n",
    "\n",
    "The training data points that exhibit an increased membership privacy risk might differ from their classes mean samples (outliers) [2]. You could check them again, see if they have the correct label, or if they exhibit any non-standard properties for the class. If so, correct them. It was also shown that points with an high influence on the decision boundary are more vulnerable to membership inference attacks [3]. Therefore, these points might be important. If you want to protect them, you might add several similar training samples as they are to the class. \n",
    "\n",
    "\n",
    "(For References, please see last box)"
   ],
   "metadata": {
    "id": "3ToJNWK5Fjzg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[1] Song, Liwei and Prateek Mittal. “Systematic Evaluation of Privacy Risks of Machine Learning Models.” ArXiv abs/2003.10595 (2020): n. pag.\n",
    "\n",
    "[2] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, HaixuTang, Carl A. Gunter, and Kai Chen. 2018.   Understanding Membership In-ferences on Well-Generalized Learning Models.CoRRabs/1802.04889 (2018).arXiv:1802.04889  http://arxiv.org/abs/1802.0\n",
    "\n",
    "[3] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. 2019.Demystifying Membership Inference Attacks in Machine Learning as a Service.IEEE Transactions on Services Computing(2019)"
   ],
   "metadata": {
    "id": "LjKjKcOtFple"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "membership_inference_black_box_rule_based_attack.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "bdaae9e836e6796a4363c97979c836477756ef9f67b8698af83fef8b5f4ebf2e"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}