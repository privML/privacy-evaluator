{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Rf_QO8flWx6_",
   "metadata": {
    "id": "Rf_QO8flWx6_"
   },
   "source": [
    "# Membership Inference Black Box Attack Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WKAYn2M4Wx7F",
   "metadata": {
    "id": "WKAYn2M4Wx7F"
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/privML/privacy-evaluator/blob/main/notebooks/membership_inference_black_box_attack.ipynb\"><img src=\"https://raw.githubusercontent.com/privML/privacy-evaluator/team1sprint5/notebooks/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/privML/privacy-evaluator/blob/main/notebooks/membership_inference_black_box_attack.ipynb\"><img src=\"https://raw.githubusercontent.com/privML/privacy-evaluator/main/notebooks/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ia0VmlzeWx7H",
   "metadata": {
    "id": "Ia0VmlzeWx7H"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this notebook, we want to show you how to use the `privacy-evaluator` tool to perform the Membership Inference Attacks Black Box Attack on both, a provided PyTorch and a provided Tensorflow model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DH6rB9KSWx7J",
   "metadata": {
    "id": "DH6rB9KSWx7J"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, you should set the notebook's runtime to use a GPU (e.g. if Colab is used go to ***Runtime > Change runtime type > Hardware accelerator***). Now we can install the `privacy-evaluator` package and import all needed modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lNU7L3GCWx7M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lNU7L3GCWx7M",
    "outputId": "94bf7d98-1d11-4009-c18a-0d7664516f53"
   },
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/privML/privacy-evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7ab468",
   "metadata": {
    "id": "9d7ab468"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "import privacy_evaluator.models.torch.dcti.dcti as torch_dcti\n",
    "import privacy_evaluator.models.tf.dcti.dcti as tf_dcti \n",
    "\n",
    "from privacy_evaluator.datasets.tf.cifar10 import TFCIFAR10\n",
    "from privacy_evaluator.datasets.torch.cifar10 import TorchCIFAR10\n",
    "\n",
    "from privacy_evaluator.classifiers.classifier import Classifier\n",
    "\n",
    "from privacy_evaluator.attacks.membership_inference.black_box import MembershipInferenceBlackBoxAttack\n",
    "from privacy_evaluator.attacks.membership_inference import MembershipInferenceAttackAnalysis\n",
    "\n",
    "from privacy_evaluator.attacks.membership_inference import MembershipInferenceAttackAnalysis\n",
    "\n",
    "from privacy_evaluator.attacks.membership_inference.data_structures.attack_input_data import AttackInputData\n",
    "from privacy_evaluator.attacks.membership_inference.data_structures.slicing import Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mUH4YQ4tWx7Q",
   "metadata": {
    "id": "mUH4YQ4tWx7Q"
   },
   "source": [
    "## Conduct Membership Inference Black Box Attacks\n",
    "\n",
    "Now we can start with conducting the Membership Inference Black Box Attacks. Therefore, we prepared two instances of the attack: one attacking a PyTorch model and attacking a TensorFlow model. For both attacks, we implemented a simple neural network trained on the CIFAR-10 dataset. For details about the provided network have a look at the following paper: https://www.scitepress.org/Papers/2018/67520/67520.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da0876",
   "metadata": {
    "id": "b2da0876"
   },
   "source": [
    "### PyTorch\n",
    "\n",
    "We start the evaluation of the PyTorch version of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aQIN6UMWx7W",
   "metadata": {
    "id": "7aQIN6UMWx7W"
   },
   "source": [
    "#### Prepare target model\n",
    "\n",
    "Now, we need to initialize our pre-trained Lightweight Deep Convolutional Neural Network (short DCTI) as a generic `Classifier`. Therefore we need to specify the loss function used to train the model (in our case the `torch.nn.CrossEntropyLoss`), the number of classes and the input shape of our CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fz4UuSxMWx7Y",
   "metadata": {
    "id": "fz4UuSxMWx7Y"
   },
   "outputs": [],
   "source": [
    "# Initalize PyTorch model as a Classifier\n",
    "target_model = Classifier(\n",
    "    torch_dcti.load_dcti(), # PyTorch DCTI \n",
    "    loss=torch.nn.CrossEntropyLoss(reduction=\"none\"), # Loss function of the PyTorch model\n",
    "    nb_classes=TorchCIFAR10.N_CLASSES, # Number of classes of the CIFAR10 dataset\n",
    "    input_shape=TorchCIFAR10.INPUT_SHAPE # Input shape of the CIFAR10 dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sv_fGg8jWx7S",
   "metadata": {
    "id": "sv_fGg8jWx7S"
   },
   "source": [
    "#### Load CIFAR10 Dataset\n",
    "\n",
    "Before we can start to conduct the membership inference attacks, we need to load the dataset. The CIFAR10 dataset needs to be preprocessed in a specific manner to work for the PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcae11d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcae11d8",
    "outputId": "7e9990f7-4fc7-4d55-9535-b5f6f170ced2"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 dataset as numpy array\n",
    "x_train, y_train, x_test, y_test = TorchCIFAR10.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7761c0c",
   "metadata": {
    "id": "c7761c0c"
   },
   "source": [
    "#### Perform Membership Inference Black Box Attack\n",
    "\n",
    "We want to attack our target model with the Membership Inference  Black Box Attack. Thus, we initialize the attack with the target model and a dataset used to fit the attack model. The dataset consists of two different sets. The first contains the data (`x_train`) and its corresponding labels (`y_train`) which were used to train the target model. The second contains the data (`x_test`) and its corresponding labels (`y_test`) which were not part of the training process of the target model. After the initialization, we first need to fit the attack model before we can attack the target model. To attack certain data points, we simply input them into the `attack()` method. The result of the attack is an array holding the inferred membership status, 1 indicates a member and 0 indicates non-member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9e4799",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd9e4799",
    "outputId": "22a5cde1-d7c0-48bc-dd3a-8732cb881e72"
   },
   "outputs": [],
   "source": [
    "attack = MembershipInferenceBlackBoxAttack(target_model)\n",
    "attack.fit(x_train[:100], y_train[:100], x_test[:100], y_test[:100])\n",
    "attack.attack(x_train[:100], y_train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5770d67d",
   "metadata": {
    "id": "5770d67d"
   },
   "source": [
    "#### Get machine-readable attack statistics\n",
    "\n",
    "Besides the inferred membership status, we can create more general statistics. To do so, we generate an attack output by providing again the data points which should be attacked and the correct inferred membership labels (in this case all attacked data points are part of the training dataset and thus for all of them a membership should be predicted by the attack model). As result, we get the accuracy, the train-to-test accuracy gap and the train-to-test ratio for the target model and the accuracy for the attack model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa9ed6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "8eaa9ed6",
    "outputId": "ac69fa2a-3c0c-46db-b9e7-d2ac084f682b"
   },
   "outputs": [],
   "source": [
    "output = attack.attack_output(\n",
    "    x_train[:100], \n",
    "    y_train[:100], \n",
    "    np.ones((100,))\n",
    ")\n",
    "\n",
    "output.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dRPu6e955n_B",
   "metadata": {
    "id": "dRPu6e955n_B"
   },
   "source": [
    "#### Explanation of the outcome:\n",
    "\n",
    "Attack Model Accuracy:\n",
    "The attack model accuracy specifies how well the membership attack model performs in predicting if a given data point was used for training the target model. Since we have a two-class classification problem that the attack model solves (member or non-member), the lowest possible accuracy is 50% (random guessing for each sample). The best accuracy is at 100% if the model predicts every data point is sees right as member or non-member.\n",
    "\n",
    "Train-Test-Gap (difference):\n",
    "If your model has a train-test-gap larger than 5%, this could be a sign that your model overfits. Overfitting can be beneficial for successful membership inference attacks [1]. Therefore, you might want to reduce it by introducing regularization methods in your training, or using specific privacy methods [2,3], such as Differential Privacy [4].\n",
    "\n",
    "(For References, please see last box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fpycszf2JBfw",
   "metadata": {
    "id": "fpycszf2JBfw"
   },
   "source": [
    "#### Perpare attack analysis\n",
    "\n",
    "Next, we prepare our attack analysis. To initialize our attack analysis we define the Membership Inference Attack method we want to perform (in this case we use the `MembershipInferenceBlackBoxAttack`) and the Attack Input Data. The Attack Input Data consists of two different sets. The first contains the data (`x_train`) and its corresponding labels (`y_train`) which were used to train the target model. The second contains the data (`x_test`) and its corresponding labels (`y_test`) which were not part of the training process of the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h7CAm0_aJD05",
   "metadata": {
    "id": "h7CAm0_aJD05"
   },
   "outputs": [],
   "source": [
    "attack_analysis = MembershipInferenceAttackAnalysis(\n",
    "    MembershipInferenceBlackBoxAttack, \n",
    "    AttackInputData(\n",
    "        x_train[:100], \n",
    "        y_train[:100], \n",
    "        x_test[:100], \n",
    "        y_test[:100]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H52Z0hJyJHj6",
   "metadata": {
    "id": "H52Z0hJyJHj6"
   },
   "source": [
    "#### Define the slicing\n",
    "\n",
    "Now we can define the slicing for our analysis. The slicing defines how the data will be sliced. Each slice will then be analysed by the attack separately, so see if the inference works better on certain parts of data than on others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Gk9F4EiCJJGq",
   "metadata": {
    "id": "Gk9F4EiCJJGq"
   },
   "outputs": [],
   "source": [
    "slicing = Slicing(\n",
    "    entire_dataset=True, \n",
    "    by_class=True, \n",
    "    by_classification_correctness=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yUEf6yXOJLUm",
   "metadata": {
    "id": "yUEf6yXOJLUm"
   },
   "source": [
    "#### Perform Membership Inference Attack Analysis\n",
    "\n",
    "Finally, we can perform our Membership Inference Attack Analysis. Therefore, we input the target model, the data that should be analysed, the membership labels (i.e. the labels which correctly describe if a data point is a member of the training dataset or not) and the splicing specification into the `analyse()` method. As a result, we get for each slice the indices of the corresponding data points, a human-readable description of the slice and the advantage score of the Membership Inference Attack (for more details about the advantage score, please read the following paper: https://arxiv.org/abs/1709.01604)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AUTnIKUEJM8R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUTnIKUEJM8R",
    "outputId": "967b0d01-c6df-41ac-c688-fb39d0b421fc",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = attack_analysis.analyse(\n",
    "    target_model, \n",
    "    np.concatenate((x_train[:100], x_test[:100])), \n",
    "    np.concatenate((y_train[:100], y_test[:100])), \n",
    "    np.concatenate((np.ones(len(x_train[:100])), np.zeros(len(x_test[:100])))), \n",
    "    slicing\n",
    ")\n",
    "\n",
    "print(\"\\n\".join((str(r) for r in result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K5yIBZEGJQxi",
   "metadata": {
    "id": "K5yIBZEGJQxi"
   },
   "source": [
    "#### Explanation of the outcome:\n",
    "##### Attacker Advantage:\n",
    "The attacker advantageis a score that relies on comparing the model output on member and non-member data points. The model outputs are probability values over all classes, and they are often different on member and non-member data points. Usually, the model is more confident on member data points, because it has seen them during training. When trying to find a threshold value to tell apart member and non-member samples by their different model outputs, the attacker has interest in finding the best ratio between false positives “fpr” (non-members that are classified as members) and true positives “tpr” (members that are correctly identifies as members). \n",
    "\n",
    "This best ratio is calculated as the max(tpr-fpr) over all threshold values and represents the attacker advantage. \n",
    "\n",
    "##### Slicing: Incorrectly classified:\n",
    "It is normal that the attacker is more successful to deduce membership on incorrectly classified samples than on correctly classified ones. This results from the fact, that model predictions are often better on training than on test data points, whereby your attack model might learn to predict incorrectly classified samples as non-members. If your model overfits the training data, this assumption might hold true often enough to make the attack seem more successful on this slice. If you wish to reduce that, pay attention to reducing your model’s overfitting.\n",
    "\n",
    "##### Slicing: Specific classes more vulnerable: \n",
    "It seems that the membership inference attack is more successful on your class X than on the other classes. Research has shown that the class distribution (and also the distribution of data points within one class) are factors that influence the vulnerability of a class for membership inference attacks [5].\n",
    "\n",
    "Also, small classes (belonging to minority groups) can be more prone to membership inference attacks [6]. One reason for this could be, that there is less data for that class, and therefore, the model overfits within this class. It might make sense to look into the vulnerable classes of your model again, and maybe add more data to them, use private synthetic data, or introduce privacy methods like Differential Privacy [6]. Attention, the use of Differential Privacy could have a negative influence on the performance of your model for the minority classes.\n",
    "\n",
    "\n",
    "(For References, please see last box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xG4nYKGzWx7i",
   "metadata": {
    "id": "xG4nYKGzWx7i"
   },
   "source": [
    "### TensorFlow\n",
    "\n",
    "Now we do the same with the TensorFlow model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_7ty54AKWx7k",
   "metadata": {
    "id": "_7ty54AKWx7k"
   },
   "source": [
    "#### Prepare target model\n",
    "\n",
    "Now, we need to initialize our pre-trained Lightweight Deep Convolutional Neural Network (short DCTI) as a generic `Classifier`. Therefore we need to specify the loss function used to train the model (in our case the `tf.keras.losses.CategoricalCrossentropy`), the number of classes and the input shape of our CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uhrn5znOWx7k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uhrn5znOWx7k",
    "outputId": "e8162ec6-d35b-4990-b364-dd30d1dfa5ca"
   },
   "outputs": [],
   "source": [
    "# Initalize TensorFlow target model\n",
    "target_model = Classifier(\n",
    "    tf_dcti.load_dcti(), # TensorFlow DCTI\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(), # Loss function of the TensorFlow target model\n",
    "    nb_classes=TFCIFAR10.N_CLASSES, # Number of classes of the CIFAR10 dataset\n",
    "    input_shape=TFCIFAR10.INPUT_SHAPE # Input shape of the CIFAR10 dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UBfeudg_Wx7i",
   "metadata": {
    "id": "UBfeudg_Wx7i"
   },
   "source": [
    "#### Load CIFAR10 Dataset\n",
    "\n",
    "Again, before we can start to conduct the membership inference attacks, we need to load the dataset. The CIFAR10 dataset needs to be preprocessed in a specific manner to work for the TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kAJgfSgoWx7j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kAJgfSgoWx7j",
    "outputId": "c5bc9052-7c5a-42ae-b392-c49499a6d9d3"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR10 dataset as numpy array\n",
    "x_train, y_train, x_test, y_test = TFCIFAR10.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mLC_hmb8Wx7l",
   "metadata": {
    "id": "mLC_hmb8Wx7l"
   },
   "source": [
    "#### Perform Membership Inference Black Box Attack\n",
    "\n",
    "First, we want to attack our target model with the Membership Inference  Black Box Attack. Thus, we initialize the attack with the target model and a dataset used to fit the attack model. The dataset consists of two different sets. The first contains the data (`x_train`) and its corresponding labels (`y_train`) which were used to train the target model. The second contains the data (`x_test`) and its corresponding labels (`y_test`) which were not part of the training process of the target model. After the initialization, we first need to fit the attack model before we can attack the target model. To attack certain data points, we simply input them into the `attack()` method. The result of the attack is an array holding the inferred membership status, 1 indicates a member and 0 indicates non-member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QLddiPbkWx7l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QLddiPbkWx7l",
    "outputId": "55bbd171-229b-41fd-8a00-8e45ecc4bf03"
   },
   "outputs": [],
   "source": [
    "attack = MembershipInferenceBlackBoxAttack(target_model)\n",
    "attack.fit(x_train[:100], y_train[:100], x_test[:100], y_test[:100])\n",
    "attack.attack(x_train[:100], y_train[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bnE8ETvRWx7m",
   "metadata": {
    "id": "bnE8ETvRWx7m"
   },
   "source": [
    "#### Get machine-readable attack statistics\n",
    "\n",
    "Besides the inferred membership status, we can create more general statistics. To do so, we generate an attack output by providing again the data points which should be attacked and the correct inferred membership labels (in this case all attacked data points are part of the training dataset and thus for all of them a membership should be predicted by the attack model). As result, we get the accuracy, the train-to-test accuracy gap and the train-to-test ratio for the target model and the accuracy for the attack model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ysx92hInWx7n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "ysx92hInWx7n",
    "outputId": "a8a6f04a-53b4-4c70-82e7-115aea79646d"
   },
   "outputs": [],
   "source": [
    "output = attack.attack_output(\n",
    "    x_train[:100], \n",
    "    y_train[:100], \n",
    "    np.ones((100,))\n",
    ")\n",
    "\n",
    "output.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZnNgIXJC7ppu",
   "metadata": {
    "id": "ZnNgIXJC7ppu"
   },
   "source": [
    "#### Explanation of the outcome:\n",
    "\n",
    "Attack Model Accuracy:\n",
    "The attack model accuracy specifies how well the membership attack model performs in predicting if a given data point was used for training the target model. Since we have a two-class classification problem that the attack model solves (member or non-member), the lowest possible accuracy is 50% (random guessing for each sample). The best accuracy is at 100% if the model predicts every data point is sees right as member or non-member.\n",
    "\n",
    "Train-Test-Gap (difference):\n",
    "If your model has a train-test-gap larger than 5%, this could be a sign that your model overfits. Overfitting can be beneficial for successful membership inference attacks [1]. Therefore, you might want to reduce it by introducing regularization methods in your training, or using specific privacy methods [2,3], such as Differential Privacy [4].\n",
    "\n",
    "(For References, please see last box)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mrnYkSK0JqNs",
   "metadata": {
    "id": "mrnYkSK0JqNs"
   },
   "source": [
    "#### Perpare attack analysis\n",
    "\n",
    "Next, we prepare our attack analysis. To initialize our attack analysis we define the Membership Inference Attack method we want to perform (in this case we use the `MembershipInferenceBlackBoxAttack`) and the Attack Input Data. The Attack Input Data consists of two different sets. The first contains the data (`x_train`) and its corresponding labels (`y_train`) which were used to train the target model. The second contains the data (`x_test`) and its corresponding labels (`y_test`) which were not part of the training process of the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gEyz9CDkJr1Z",
   "metadata": {
    "id": "gEyz9CDkJr1Z"
   },
   "outputs": [],
   "source": [
    "analysis = MembershipInferenceAttackAnalysis(\n",
    "    MembershipInferenceBlackBoxAttack, \n",
    "    AttackInputData(\n",
    "        x_train[:100], \n",
    "        y_train[:100], \n",
    "        x_test[:100], \n",
    "        y_test[:100]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9UEL4XUBJtUP",
   "metadata": {
    "id": "9UEL4XUBJtUP"
   },
   "source": [
    "#### Define the slicing\n",
    "\n",
    "Now we can define the slicing for our analysis. The slicing defines how the data will be sliced. Each slice will then be analysed by the attack separately, so see if the inference works better on certain parts of data than on others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YeGUtkbKJvmv",
   "metadata": {
    "id": "YeGUtkbKJvmv"
   },
   "outputs": [],
   "source": [
    "slicing = Slicing(\n",
    "    entire_dataset=True, \n",
    "    by_class=True, \n",
    "    by_classification_correctness=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cT3cZVFFJxJq",
   "metadata": {
    "id": "cT3cZVFFJxJq"
   },
   "source": [
    "#### Perform Membership Inference Attack Analysis\n",
    "\n",
    "Finally, we can perform our Membership Inference Attack Analysis. Therefore, we input the target model, the data that should be analysed, the membership labels (i.e. the labels which correctly describe if a data point is a member of the training dataset or not) and the splicing specification into the `analyse()` method. As a result, we get for each slice the indices of the corresponding data points, a human-readable description of the slice and the advantage score of the Membership Inference Attack (for more details about the advantage score, please read the following paper: https://arxiv.org/abs/1709.01604)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gFQuFLedJzEo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFQuFLedJzEo",
    "outputId": "3a86763a-c4af-4844-c659-800a7eab2211",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = analysis.analyse(\n",
    "    target_model, \n",
    "    np.concatenate((x_train[:100], x_test[:100])), \n",
    "    np.concatenate((y_train[:100], y_test[:100])), \n",
    "    np.concatenate((np.ones(len(x_train[:100])), np.zeros(len(x_test[:100])))), \n",
    "    slicing\n",
    ")\n",
    "\n",
    "print(\"\\n\".join((str(r) for r in result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V9n1Y1AuJ0YY",
   "metadata": {
    "id": "V9n1Y1AuJ0YY"
   },
   "source": [
    "#### Explanation of the outcome:\n",
    "##### Attacker Advantage:\n",
    "The attacker advantageis a score that relies on comparing the model output on member and non-member data points. The model outputs are probability values over all classes, and they are often different on member and non-member data points. Usually, the model is more confident on member data points, because it has seen them during training. When trying to find a threshold value to tell apart member and non-member samples by their different model outputs, the attacker has interest in finding the best ratio between false positives “fpr” (non-members that are classified as members) and true positives “tpr” (members that are correctly identifies as members). \n",
    "\n",
    "This best ratio is calculated as the max(tpr-fpr) over all threshold values and represents the attacker advantage. \n",
    "\n",
    "##### Slicing: Incorrectly classified:\n",
    "It is normal that the attacker is more successful to deduce membership on incorrectly classified samples than on correctly classified ones. This results from the fact, that model predictions are often better on training than on test data points, whereby your attack model might learn to predict incorrectly classified samples as non-members. If your model overfits the training data, this assumption might hold true often enough to make the attack seem more successful on this slice. If you wish to reduce that, pay attention to reducing your model’s overfitting.\n",
    "\n",
    "##### Slicing: Specific classes more vulnerable: \n",
    "It seems that the membership inference attack is more successful on your class X than on the other classes. Research has shown that the class distribution (and also the distribution of data points within one class) are factors that influence the vulnerability of a class for membership inference attacks [5].\n",
    "\n",
    "Also, small classes (belonging to minority groups) can be more prone to membership inference attacks [6]. One reason for this could be, that there is less data for that class, and therefore, the model overfits within this class. It might make sense to look into the vulnerable classes of your model again, and maybe add more data to them, use private synthetic data, or introduce privacy methods like Differential Privacy [6]. Attention, the use of Differential Privacy could have a negative influence on the performance of your model for the minority classes.\n",
    "\n",
    "\n",
    "(For References, please see last box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DmyWRprb842M",
   "metadata": {
    "id": "DmyWRprb842M"
   },
   "source": [
    "[1]S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. \\Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting\". In: 2018 IEEE 31st Computer Security Foundations Symposium (CSF). July 2018, pp. 268{282. doi:10.1109/CSF.2018.00027.\n",
    "\n",
    "[2] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-bership Inference Attacks Against Machine Learning Models. In2017 IEEE Sym-posium on Security and Privacy (SP). 3–18.\n",
    "\n",
    "[3] Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Machine Learning withMembership Privacy Using Adversarial Regularization. InProceedings of the 2018ACM SIGSAC Conference on Computer and Communications Security(Toronto,Canada)(CCS ’18). Association for Computing Machinery, New York, NY, USA,634–64\n",
    "\n",
    "[4] Cynthia Dwork. 2006.  Differential Privacy. InAutomata, Languages and Pro-gramming, Michele Bugliesi, Bart Preneel, Vladimiro Sassone, and Ingo Wegener(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg\n",
    "\n",
    "[5] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. 2019.Demystifying Membership Inference Attacks in Machine Learning as a Service.IEEE Transactions on Services Computing(2019)\n",
    "\n",
    "[6] Suriyakumar, Vinith M., Nicolas Papernot, Anna Goldenberg, and Marzyeh Ghassemi. \"Chasing Your Long Tails: Differentially Private Prediction in Health Care Settings.\" In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pp. 723-734. 2021.\n",
    "\n",
    "[7] Yunhui Long, Vincent Bindschaedler, Lei Wang, Diyue Bu, Xiaofeng Wang, HaixuTang, Carl A. Gunter, and Kai Chen. 2018.   Understanding Membership In-ferences on Well-Generalized Learning Models.CoRRabs/1802.04889 (2018).arXiv:1802.04889  http://arxiv.org/abs/1802.0\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "membership_inference_black_box_attack.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "bdaae9e836e6796a4363c97979c836477756ef9f67b8698af83fef8b5f4ebf2e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
